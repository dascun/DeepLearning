{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "=================================================================\n",
      "Total params: 4,489,216\n",
      "Trainable params: 4,489,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8192)              4202496   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 128, 128, 3)       867       \n",
      "=================================================================\n",
      "Total params: 4,644,739\n",
      "Trainable params: 4,644,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2723 samples, validate on 303 samples\n",
      "Epoch 1/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 36771.0307Epoch 00000: val_loss improved from inf to 4325.97461, saving model to hackathon_autoencoder.00-4325.97.h5\n",
      "2723/2723 [==============================] - 312s 115ms/step - loss: 36733.6205 - val_loss: 4325.9746\n",
      "Epoch 2/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 3137.3038Epoch 00001: val_loss improved from 4325.97461 to 3884.14432, saving model to hackathon_autoencoder.01-3884.14.h5\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 3136.7302 - val_loss: 3884.1443\n",
      "Epoch 3/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 2874.6767Epoch 00002: val_loss improved from 3884.14432 to 3504.27198, saving model to hackathon_autoencoder.02-3504.27.h5\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 2877.5261 - val_loss: 3504.2720\n",
      "Epoch 4/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 2429.3326Epoch 00003: val_loss improved from 3504.27198 to 2314.95175, saving model to hackathon_autoencoder.03-2314.95.h5\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 2428.1214 - val_loss: 2314.9517\n",
      "Epoch 5/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 2256.4181Epoch 00004: val_loss improved from 2314.95175 to 2162.64414, saving model to hackathon_autoencoder.04-2162.64.h5\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 2256.8851 - val_loss: 2162.6441\n",
      "Epoch 6/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 2205.3041Epoch 00005: val_loss improved from 2162.64414 to 2029.85386, saving model to hackathon_autoencoder.05-2029.85.h5\n",
      "2723/2723 [==============================] - 300s 110ms/step - loss: 2207.9775 - val_loss: 2029.8539\n",
      "Epoch 7/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 2152.9027Epoch 00006: val_loss improved from 2029.85386 to 1961.12715, saving model to hackathon_autoencoder.06-1961.13.h5\n",
      "2723/2723 [==============================] - 300s 110ms/step - loss: 2152.8238 - val_loss: 1961.1271\n",
      "Epoch 8/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1986.0672Epoch 00007: val_loss improved from 1961.12715 to 1831.12412, saving model to hackathon_autoencoder.07-1831.12.h5\n",
      "2723/2723 [==============================] - 300s 110ms/step - loss: 1986.6435 - val_loss: 1831.1241\n",
      "Epoch 9/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1900.3245Epoch 00008: val_loss improved from 1831.12412 to 1729.06051, saving model to hackathon_autoencoder.08-1729.06.h5\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 1900.4994 - val_loss: 1729.0605\n",
      "Epoch 10/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1856.0416Epoch 00009: val_loss improved from 1729.06051 to 1654.77400, saving model to hackathon_autoencoder.09-1654.77.h5\n",
      "2723/2723 [==============================] - 300s 110ms/step - loss: 1855.6162 - val_loss: 1654.7740\n",
      "Epoch 11/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1824.2662Epoch 00010: val_loss did not improve\n",
      "2723/2723 [==============================] - 316s 116ms/step - loss: 1824.1968 - val_loss: 1724.2286\n",
      "Epoch 12/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1778.6270Epoch 00011: val_loss improved from 1654.77400 to 1637.98225, saving model to hackathon_autoencoder.11-1637.98.h5\n",
      "2723/2723 [==============================] - 305s 112ms/step - loss: 1778.5216 - val_loss: 1637.9822\n",
      "Epoch 13/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1692.3018Epoch 00012: val_loss improved from 1637.98225 to 1585.13789, saving model to hackathon_autoencoder.12-1585.14.h5\n",
      "2723/2723 [==============================] - 306s 112ms/step - loss: 1692.0192 - val_loss: 1585.1379\n",
      "Epoch 14/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1643.5884Epoch 00013: val_loss improved from 1585.13789 to 1525.08972, saving model to hackathon_autoencoder.13-1525.09.h5\n",
      "2723/2723 [==============================] - 311s 114ms/step - loss: 1643.1918 - val_loss: 1525.0897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1583.7355Epoch 00014: val_loss improved from 1525.08972 to 1495.49398, saving model to hackathon_autoencoder.14-1495.49.h5\n",
      "2723/2723 [==============================] - 315s 116ms/step - loss: 1583.3912 - val_loss: 1495.4940\n",
      "Epoch 16/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1520.5004Epoch 00015: val_loss improved from 1495.49398 to 1419.36018, saving model to hackathon_autoencoder.15-1419.36.h5\n",
      "2723/2723 [==============================] - 306s 112ms/step - loss: 1520.6933 - val_loss: 1419.3602\n",
      "Epoch 17/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1455.4744Epoch 00016: val_loss improved from 1419.36018 to 1391.11367, saving model to hackathon_autoencoder.16-1391.11.h5\n",
      "2723/2723 [==============================] - 304s 112ms/step - loss: 1455.6052 - val_loss: 1391.1137\n",
      "Epoch 18/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1407.9241Epoch 00017: val_loss did not improve\n",
      "2723/2723 [==============================] - 303s 111ms/step - loss: 1407.9795 - val_loss: 1426.0926\n",
      "Epoch 19/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1381.8133Epoch 00018: val_loss improved from 1391.11367 to 1386.37226, saving model to hackathon_autoencoder.18-1386.37.h5\n",
      "2723/2723 [==============================] - 308s 113ms/step - loss: 1381.4904 - val_loss: 1386.3723\n",
      "Epoch 20/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1353.3285Epoch 00019: val_loss improved from 1386.37226 to 1348.77839, saving model to hackathon_autoencoder.19-1348.78.h5\n",
      "2723/2723 [==============================] - 304s 112ms/step - loss: 1353.0519 - val_loss: 1348.7784\n",
      "Epoch 21/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1310.6873Epoch 00020: val_loss improved from 1348.77839 to 1299.62122, saving model to hackathon_autoencoder.20-1299.62.h5\n",
      "2723/2723 [==============================] - 304s 112ms/step - loss: 1310.5019 - val_loss: 1299.6212\n",
      "Epoch 22/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1289.2686Epoch 00021: val_loss did not improve\n",
      "2723/2723 [==============================] - 314s 115ms/step - loss: 1290.0222 - val_loss: 1384.3536\n",
      "Epoch 23/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1275.4354Epoch 00022: val_loss improved from 1299.62122 to 1261.45138, saving model to hackathon_autoencoder.22-1261.45.h5\n",
      "2723/2723 [==============================] - 314s 115ms/step - loss: 1275.5470 - val_loss: 1261.4514\n",
      "Epoch 24/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1243.3511Epoch 00023: val_loss did not improve\n",
      "2723/2723 [==============================] - 645s 237ms/step - loss: 1243.1972 - val_loss: 1338.3943\n",
      "Epoch 25/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1257.3973Epoch 00024: val_loss improved from 1261.45138 to 1252.55518, saving model to hackathon_autoencoder.24-1252.56.h5\n",
      "2723/2723 [==============================] - 365s 134ms/step - loss: 1257.0581 - val_loss: 1252.5552\n",
      "Epoch 26/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1177.6384Epoch 00025: val_loss improved from 1252.55518 to 1189.70389, saving model to hackathon_autoencoder.25-1189.70.h5\n",
      "2723/2723 [==============================] - 305s 112ms/step - loss: 1178.0206 - val_loss: 1189.7039\n",
      "Epoch 27/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1160.5201Epoch 00026: val_loss did not improve\n",
      "2723/2723 [==============================] - 305s 112ms/step - loss: 1160.5920 - val_loss: 1226.2982\n",
      "Epoch 28/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1147.7440Epoch 00027: val_loss did not improve\n",
      "2723/2723 [==============================] - 303s 111ms/step - loss: 1147.4410 - val_loss: 1267.5089\n",
      "Epoch 29/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1132.0322Epoch 00028: val_loss did not improve\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 1132.2919 - val_loss: 1216.8699\n",
      "Epoch 30/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1106.9982Epoch 00029: val_loss improved from 1189.70389 to 1150.36283, saving model to hackathon_autoencoder.29-1150.36.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 1107.1741 - val_loss: 1150.3628\n",
      "Epoch 31/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1111.1175Epoch 00030: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 1111.4031 - val_loss: 1187.6515\n",
      "Epoch 32/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1061.4220Epoch 00031: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 1061.1910 - val_loss: 1171.5126\n",
      "Epoch 33/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1032.7158Epoch 00032: val_loss improved from 1150.36283 to 1140.70772, saving model to hackathon_autoencoder.32-1140.71.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 1032.7669 - val_loss: 1140.7077\n",
      "Epoch 34/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1019.2797Epoch 00033: val_loss improved from 1140.70772 to 1113.05240, saving model to hackathon_autoencoder.33-1113.05.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 1019.4115 - val_loss: 1113.0524\n",
      "Epoch 35/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1029.3162Epoch 00034: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 1029.9694 - val_loss: 1178.9643\n",
      "Epoch 36/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 1010.9152Epoch 00035: val_loss improved from 1113.05240 to 1109.69782, saving model to hackathon_autoencoder.35-1109.70.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 1010.7809 - val_loss: 1109.6978\n",
      "Epoch 37/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 967.0266Epoch 00036: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 966.8627 - val_loss: 1111.8542\n",
      "Epoch 38/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 959.0115Epoch 00037: val_loss did not improve\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 959.4351 - val_loss: 1124.9830\n",
      "Epoch 39/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 951.4812Epoch 00038: val_loss improved from 1109.69782 to 1089.41871, saving model to hackathon_autoencoder.38-1089.42.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 951.4455 - val_loss: 1089.4187\n",
      "Epoch 40/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 950.3845Epoch 00039: val_loss improved from 1089.41871 to 1068.07900, saving model to hackathon_autoencoder.39-1068.08.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 950.6343 - val_loss: 1068.0790\n",
      "Epoch 41/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 935.5436Epoch 00040: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 935.7538 - val_loss: 1123.2022\n",
      "Epoch 42/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 898.2344Epoch 00041: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 898.1957 - val_loss: 1162.0349\n",
      "Epoch 43/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 908.4390Epoch 00042: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 908.5544 - val_loss: 1130.1871\n",
      "Epoch 44/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 873.5002Epoch 00043: val_loss improved from 1068.07900 to 1034.41563, saving model to hackathon_autoencoder.43-1034.42.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 873.4504 - val_loss: 1034.4156\n",
      "Epoch 45/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 862.9488Epoch 00044: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 862.6563 - val_loss: 1039.7967\n",
      "Epoch 46/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 839.2083Epoch 00045: val_loss improved from 1034.41563 to 1034.01065, saving model to hackathon_autoencoder.45-1034.01.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 838.9045 - val_loss: 1034.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 842.0902Epoch 00046: val_loss improved from 1034.01065 to 1020.03316, saving model to hackathon_autoencoder.46-1020.03.h5\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 841.9878 - val_loss: 1020.0332\n",
      "Epoch 48/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 818.1628Epoch 00047: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 818.2738 - val_loss: 1061.7959\n",
      "Epoch 49/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 827.5052Epoch 00048: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 827.5643 - val_loss: 1042.8332\n",
      "Epoch 50/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 805.1712Epoch 00049: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 805.0769 - val_loss: 1034.2648\n",
      "Epoch 51/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 790.9184Epoch 00050: val_loss improved from 1020.03316 to 1019.23681, saving model to hackathon_autoencoder.50-1019.24.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 790.7428 - val_loss: 1019.2368\n",
      "Epoch 52/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 779.0879Epoch 00051: val_loss improved from 1019.23681 to 984.18365, saving model to hackathon_autoencoder.51-984.18.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 779.1424 - val_loss: 984.1836\n",
      "Epoch 53/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 775.0055Epoch 00052: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 775.3656 - val_loss: 1056.1677\n",
      "Epoch 54/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 790.0143Epoch 00053: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 790.0740 - val_loss: 992.9357\n",
      "Epoch 55/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 761.7036Epoch 00054: val_loss improved from 984.18365 to 981.38637, saving model to hackathon_autoencoder.54-981.39.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 761.4370 - val_loss: 981.3864\n",
      "Epoch 56/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 734.5528Epoch 00055: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 734.8959 - val_loss: 992.8293\n",
      "Epoch 57/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 732.0617Epoch 00056: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 732.1773 - val_loss: 984.6841\n",
      "Epoch 58/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 740.1642Epoch 00057: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 740.2240 - val_loss: 983.2455\n",
      "Epoch 59/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 726.5154Epoch 00058: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 727.0190 - val_loss: 1014.0138\n",
      "Epoch 60/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 718.5569Epoch 00059: val_loss improved from 981.38637 to 945.34879, saving model to hackathon_autoencoder.59-945.35.h5\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 718.3847 - val_loss: 945.3488\n",
      "Epoch 61/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 698.5103Epoch 00060: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 698.8258 - val_loss: 965.8981\n",
      "Epoch 62/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 709.5103Epoch 00061: val_loss improved from 945.34879 to 943.29868, saving model to hackathon_autoencoder.61-943.30.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 709.2551 - val_loss: 943.2987\n",
      "Epoch 63/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 688.3641Epoch 00062: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 688.5431 - val_loss: 958.0651\n",
      "Epoch 64/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 683.4580Epoch 00063: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 683.4298 - val_loss: 1026.0594\n",
      "Epoch 65/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 674.6610Epoch 00064: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 674.7658 - val_loss: 976.5345\n",
      "Epoch 66/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 670.3652Epoch 00065: val_loss improved from 943.29868 to 939.00881, saving model to hackathon_autoencoder.65-939.01.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 670.6953 - val_loss: 939.0088\n",
      "Epoch 67/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 657.3274Epoch 00066: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 657.4025 - val_loss: 947.4829\n",
      "Epoch 68/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 655.9415Epoch 00067: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 655.9605 - val_loss: 985.1538\n",
      "Epoch 69/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 652.5925Epoch 00068: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 652.6741 - val_loss: 945.1771\n",
      "Epoch 70/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 647.1723Epoch 00069: val_loss improved from 939.00881 to 933.40752, saving model to hackathon_autoencoder.69-933.41.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 646.8870 - val_loss: 933.4075\n",
      "Epoch 71/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 639.4772Epoch 00070: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 639.6580 - val_loss: 957.9212\n",
      "Epoch 72/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 636.6546Epoch 00071: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 636.8162 - val_loss: 990.6668\n",
      "Epoch 73/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 631.5459Epoch 00072: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 631.4516 - val_loss: 975.0234\n",
      "Epoch 74/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 622.2899Epoch 00073: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 622.1909 - val_loss: 1007.0201\n",
      "Epoch 75/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 619.1056Epoch 00074: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 619.1771 - val_loss: 934.7173\n",
      "Epoch 76/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 625.3922Epoch 00075: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 625.2914 - val_loss: 937.7177\n",
      "Epoch 77/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 611.3151Epoch 00076: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 611.1587 - val_loss: 945.1088\n",
      "Epoch 78/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 592.7109Epoch 00077: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 592.7327 - val_loss: 981.9622\n",
      "Epoch 79/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 591.9319Epoch 00078: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 591.8991 - val_loss: 962.4816\n",
      "Epoch 80/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 588.9733Epoch 00079: val_loss did not improve\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 589.0705 - val_loss: 936.7909\n",
      "Epoch 81/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 575.2150Epoch 00080: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 575.2730 - val_loss: 942.3457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 577.2629Epoch 00081: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 577.5207 - val_loss: 954.3979\n",
      "Epoch 83/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 581.2510Epoch 00082: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 581.6796 - val_loss: 966.2000\n",
      "Epoch 84/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 572.8743Epoch 00083: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 572.9007 - val_loss: 962.9184\n",
      "Epoch 85/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 560.3783Epoch 00084: val_loss improved from 933.40752 to 930.73534, saving model to hackathon_autoencoder.84-930.74.h5\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 560.3559 - val_loss: 930.7353\n",
      "Epoch 86/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 543.0997Epoch 00085: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 543.0347 - val_loss: 941.7235\n",
      "Epoch 87/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 550.0899Epoch 00086: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 550.0928 - val_loss: 935.0594\n",
      "Epoch 88/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 554.1047Epoch 00087: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 554.3494 - val_loss: 973.8956\n",
      "Epoch 89/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 552.6632Epoch 00088: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 552.6480 - val_loss: 967.4944\n",
      "Epoch 90/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 551.9012Epoch 00089: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 552.1125 - val_loss: 958.8190\n",
      "Epoch 91/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 541.0820Epoch 00090: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 541.1629 - val_loss: 962.4521\n",
      "Epoch 92/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 537.0434Epoch 00091: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 537.2631 - val_loss: 964.2935\n",
      "Epoch 93/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 530.8810Epoch 00092: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 530.7392 - val_loss: 970.2091\n",
      "Epoch 94/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 527.1747Epoch 00093: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 527.2519 - val_loss: 947.2925\n",
      "Epoch 95/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 524.2113Epoch 00094: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 524.1864 - val_loss: 971.9802\n",
      "Epoch 96/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 518.1183Epoch 00095: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 518.0141 - val_loss: 942.7478\n",
      "Epoch 97/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 508.2923Epoch 00096: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 508.2913 - val_loss: 961.1643\n",
      "Epoch 98/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 518.0890Epoch 00097: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 518.1811 - val_loss: 955.6153\n",
      "Epoch 99/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 500.0920Epoch 00098: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 500.0827 - val_loss: 1005.2508\n",
      "Epoch 100/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 506.5205Epoch 00099: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 506.6616 - val_loss: 1001.2974\n",
      "Epoch 101/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 504.6425Epoch 00100: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 504.6690 - val_loss: 952.4241\n",
      "Epoch 102/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 495.4848Epoch 00101: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 495.5048 - val_loss: 1004.4103\n",
      "Epoch 103/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 486.9833Epoch 00102: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 486.9112 - val_loss: 998.0367\n",
      "Epoch 104/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 479.7551Epoch 00103: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 479.8752 - val_loss: 999.4345\n",
      "Epoch 105/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 489.7877Epoch 00104: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 489.7400 - val_loss: 950.2121\n",
      "Epoch 106/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 474.5317Epoch 00105: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 474.5311 - val_loss: 979.7958\n",
      "Epoch 107/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 484.9585Epoch 00106: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 484.7743 - val_loss: 979.6465\n",
      "Epoch 108/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 476.1356Epoch 00107: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 476.2839 - val_loss: 977.2727\n",
      "Epoch 109/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 482.1149Epoch 00108: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 482.0475 - val_loss: 956.6549\n",
      "Epoch 110/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 478.1924Epoch 00109: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 478.0352 - val_loss: 964.9160\n",
      "Epoch 111/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 463.5284Epoch 00110: val_loss did not improve\n",
      "2723/2723 [==============================] - 302s 111ms/step - loss: 463.5171 - val_loss: 945.4594\n",
      "Epoch 112/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 456.8832Epoch 00111: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 456.8330 - val_loss: 968.5368\n",
      "Epoch 113/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 449.7193Epoch 00112: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 449.8122 - val_loss: 965.6758\n",
      "Epoch 114/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 449.4838Epoch 00113: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 449.4789 - val_loss: 967.4300\n",
      "Epoch 115/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 442.1204Epoch 00114: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 442.2625 - val_loss: 958.6755\n",
      "Epoch 116/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 450.8009Epoch 00115: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 450.8876 - val_loss: 961.8843\n",
      "Epoch 117/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 441.6005Epoch 00116: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 441.6785 - val_loss: 992.5260\n",
      "Epoch 118/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 436.7114Epoch 00117: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 436.7021 - val_loss: 982.1747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 426.2050Epoch 00118: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 426.5314 - val_loss: 966.6099\n",
      "Epoch 120/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 423.2360Epoch 00119: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 423.2940 - val_loss: 985.2424\n",
      "Epoch 121/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 427.9855Epoch 00120: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 427.8734 - val_loss: 976.3655\n",
      "Epoch 122/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 429.3741Epoch 00121: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 429.3739 - val_loss: 974.8788\n",
      "Epoch 123/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 424.4347Epoch 00122: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 424.6106 - val_loss: 969.0575\n",
      "Epoch 124/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 410.9995Epoch 00123: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 411.0250 - val_loss: 971.8157\n",
      "Epoch 125/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 413.3309Epoch 00124: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 413.3152 - val_loss: 966.4376\n",
      "Epoch 126/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 413.5656Epoch 00125: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 413.6142 - val_loss: 981.4499\n",
      "Epoch 127/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 409.7027Epoch 00126: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 409.6383 - val_loss: 986.2046\n",
      "Epoch 128/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 400.8078Epoch 00127: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 400.8680 - val_loss: 983.0074\n",
      "Epoch 129/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 411.1488Epoch 00128: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 411.1390 - val_loss: 989.1250\n",
      "Epoch 130/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 401.6695Epoch 00129: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 401.7931 - val_loss: 977.0758\n",
      "Epoch 131/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 400.8613Epoch 00130: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 400.8511 - val_loss: 1040.4984\n",
      "Epoch 132/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 398.8511Epoch 00131: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 398.8629 - val_loss: 994.3606\n",
      "Epoch 133/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 392.1245Epoch 00132: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 392.0027 - val_loss: 994.0026\n",
      "Epoch 134/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 392.8374Epoch 00133: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 393.1509 - val_loss: 1025.6558\n",
      "Epoch 135/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 400.8280Epoch 00134: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 401.0162 - val_loss: 1020.4205\n",
      "Epoch 136/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 392.0535Epoch 00135: val_loss did not improve\n",
      "2723/2723 [==============================] - 296s 109ms/step - loss: 392.0515 - val_loss: 989.3279\n",
      "Epoch 137/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 388.3370Epoch 00136: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 388.2820 - val_loss: 999.9819\n",
      "Epoch 138/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 382.9366Epoch 00137: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 382.9460 - val_loss: 1014.8614\n",
      "Epoch 139/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 384.4609Epoch 00138: val_loss did not improve\n",
      "2723/2723 [==============================] - 303s 111ms/step - loss: 384.4340 - val_loss: 1037.1382\n",
      "Epoch 140/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 387.3262Epoch 00139: val_loss did not improve\n",
      "2723/2723 [==============================] - 302s 111ms/step - loss: 387.2711 - val_loss: 1012.7187\n",
      "Epoch 141/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 381.7660Epoch 00140: val_loss did not improve\n",
      "2723/2723 [==============================] - 350s 129ms/step - loss: 381.7401 - val_loss: 1036.1057\n",
      "Epoch 142/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 369.3301Epoch 00141: val_loss did not improve\n",
      "2723/2723 [==============================] - 685s 252ms/step - loss: 369.2870 - val_loss: 1001.6746\n",
      "Epoch 143/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 372.9398Epoch 00142: val_loss did not improve\n",
      "2723/2723 [==============================] - 686s 252ms/step - loss: 372.8659 - val_loss: 1055.3099\n",
      "Epoch 144/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 379.0648Epoch 00143: val_loss did not improve\n",
      "2723/2723 [==============================] - 687s 252ms/step - loss: 378.9817 - val_loss: 1002.1177\n",
      "Epoch 145/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 379.6981Epoch 00144: val_loss did not improve\n",
      "2723/2723 [==============================] - 686s 252ms/step - loss: 379.6317 - val_loss: 1008.2644\n",
      "Epoch 146/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 366.4618Epoch 00145: val_loss did not improve\n",
      "2723/2723 [==============================] - 687s 252ms/step - loss: 366.4775 - val_loss: 1012.1781\n",
      "Epoch 147/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 365.0295 Epoch 00146: val_loss did not improve\n",
      "2723/2723 [==============================] - 835s 307ms/step - loss: 365.0248 - val_loss: 1020.6682\n",
      "Epoch 148/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 362.4406Epoch 00147: val_loss did not improve\n",
      "2723/2723 [==============================] - 656s 241ms/step - loss: 362.3869 - val_loss: 1005.0775\n",
      "Epoch 149/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 359.5227Epoch 00148: val_loss did not improve\n",
      "2723/2723 [==============================] - 639s 235ms/step - loss: 359.5716 - val_loss: 1029.3875\n",
      "Epoch 150/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 370.3601Epoch 00149: val_loss did not improve\n",
      "2723/2723 [==============================] - 605s 222ms/step - loss: 370.3060 - val_loss: 1012.2747\n",
      "Epoch 151/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 368.6032Epoch 00150: val_loss did not improve\n",
      "2723/2723 [==============================] - 671s 247ms/step - loss: 368.6268 - val_loss: 1030.7281\n",
      "Epoch 152/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 368.1253Epoch 00151: val_loss did not improve\n",
      "2723/2723 [==============================] - 646s 237ms/step - loss: 368.1587 - val_loss: 1021.4546\n",
      "Epoch 153/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 359.0005Epoch 00152: val_loss did not improve\n",
      "2723/2723 [==============================] - 680s 250ms/step - loss: 358.9126 - val_loss: 1015.7723\n",
      "Epoch 154/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 352.9601Epoch 00153: val_loss did not improve\n",
      "2723/2723 [==============================] - 515s 189ms/step - loss: 353.0025 - val_loss: 1044.4069\n",
      "Epoch 155/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 350.5172Epoch 00154: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 350.5043 - val_loss: 1027.9821\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720/2723 [============================>.] - ETA: 0s - loss: 349.8034Epoch 00155: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 349.8317 - val_loss: 1026.7277\n",
      "Epoch 157/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 357.3101Epoch 00156: val_loss did not improve\n",
      "2723/2723 [==============================] - 299s 110ms/step - loss: 357.5758 - val_loss: 1025.3214\n",
      "Epoch 158/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 360.6481Epoch 00157: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 360.6673 - val_loss: 1007.9116\n",
      "Epoch 159/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 345.1160Epoch 00158: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 345.1236 - val_loss: 1026.9686\n",
      "Epoch 160/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 341.0155Epoch 00159: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 341.0043 - val_loss: 1020.3631\n",
      "Epoch 161/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 343.7902Epoch 00160: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 343.7560 - val_loss: 1027.2995\n",
      "Epoch 162/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 341.1225Epoch 00161: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 341.1933 - val_loss: 1027.8028\n",
      "Epoch 163/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 337.8504Epoch 00162: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 337.8958 - val_loss: 1025.3633\n",
      "Epoch 164/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 343.5165Epoch 00163: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 343.4843 - val_loss: 1046.7251\n",
      "Epoch 165/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 343.4514Epoch 00164: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 343.4083 - val_loss: 1026.9382\n",
      "Epoch 166/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 346.3409Epoch 00165: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 346.3535 - val_loss: 1044.8683\n",
      "Epoch 167/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 336.3818Epoch 00166: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 336.3904 - val_loss: 1031.2676\n",
      "Epoch 168/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 328.7463Epoch 00167: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 328.6583 - val_loss: 1040.2930\n",
      "Epoch 169/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 334.3919Epoch 00168: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 334.4262 - val_loss: 1019.3386\n",
      "Epoch 170/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 338.8287Epoch 00169: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 338.7303 - val_loss: 1021.1666\n",
      "Epoch 171/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 329.7413Epoch 00170: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 329.6026 - val_loss: 1043.1036\n",
      "Epoch 172/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 328.8963Epoch 00171: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 328.8085 - val_loss: 1041.0573\n",
      "Epoch 173/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 335.3490Epoch 00172: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 335.3009 - val_loss: 1048.6245\n",
      "Epoch 174/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 336.2440Epoch 00173: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 336.1847 - val_loss: 1038.6116\n",
      "Epoch 175/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 329.2982Epoch 00174: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 329.5256 - val_loss: 1038.4675\n",
      "Epoch 176/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 326.3660Epoch 00175: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 326.3600 - val_loss: 1033.7070\n",
      "Epoch 177/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 329.0312Epoch 00176: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 329.1721 - val_loss: 1046.4704\n",
      "Epoch 178/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 326.2770Epoch 00177: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 326.1926 - val_loss: 1027.4410\n",
      "Epoch 179/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 320.7552Epoch 00178: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 320.6972 - val_loss: 1016.9468\n",
      "Epoch 180/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 318.4560Epoch 00179: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 318.5443 - val_loss: 1051.2038\n",
      "Epoch 181/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 321.0268Epoch 00180: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 321.0560 - val_loss: 1057.3246\n",
      "Epoch 182/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 322.8898Epoch 00181: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 322.8198 - val_loss: 1047.2339\n",
      "Epoch 183/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 326.7338Epoch 00182: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 326.7537 - val_loss: 1050.0301\n",
      "Epoch 184/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 325.0369Epoch 00183: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 325.0326 - val_loss: 1097.3159\n",
      "Epoch 185/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 323.1692Epoch 00184: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 323.1577 - val_loss: 1041.9991\n",
      "Epoch 186/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 311.4988Epoch 00185: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 311.4291 - val_loss: 1043.5721\n",
      "Epoch 187/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 311.0620Epoch 00186: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 311.1566 - val_loss: 1047.5657\n",
      "Epoch 188/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 308.4286Epoch 00187: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 308.4552 - val_loss: 1048.5978\n",
      "Epoch 189/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 309.1542Epoch 00188: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 309.1834 - val_loss: 1049.9858\n",
      "Epoch 190/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 305.7899Epoch 00189: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 305.8718 - val_loss: 1049.4158\n",
      "Epoch 191/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 308.5652Epoch 00190: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 308.6501 - val_loss: 1051.0364\n",
      "Epoch 192/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 308.5357Epoch 00191: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 308.5483 - val_loss: 1077.3223\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720/2723 [============================>.] - ETA: 0s - loss: 318.3689Epoch 00192: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 318.2740 - val_loss: 1045.5874\n",
      "Epoch 194/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 307.0606Epoch 00193: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 306.9699 - val_loss: 1058.3514\n",
      "Epoch 195/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 298.4331Epoch 00194: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 298.4424 - val_loss: 1047.0505\n",
      "Epoch 196/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 297.2532Epoch 00195: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 297.2806 - val_loss: 1058.1522\n",
      "Epoch 197/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 300.0581Epoch 00196: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 300.0449 - val_loss: 1088.2746\n",
      "Epoch 198/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 305.5255Epoch 00197: val_loss did not improve\n",
      "2723/2723 [==============================] - 298s 109ms/step - loss: 305.4849 - val_loss: 1048.2089\n",
      "Epoch 199/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 306.5861Epoch 00198: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 306.5782 - val_loss: 1078.9992\n",
      "Epoch 200/200\n",
      "2720/2723 [============================>.] - ETA: 0s - loss: 298.9580Epoch 00199: val_loss did not improve\n",
      "2723/2723 [==============================] - 297s 109ms/step - loss: 298.8727 - val_loss: 1047.6184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0ea0e4ef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as k\n",
    "from keras.layers import merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Activation\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import rotate as rot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def my_autoencode(img_shape, code_size=32):\n",
    "    H,W,C = img_shape\n",
    "    \n",
    "    # encoder\n",
    "    encoder = k.models.Sequential()\n",
    "    encoder.add(k.layers.InputLayer(img_shape))\n",
    "    encoder.add(k.layers.Conv2D(32, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.AveragePooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Flatten())\n",
    "    encoder.add(k.layers.Dense(512, activation='elu'))\n",
    "    encoder.add(k.layers.Dense(256, activation='elu'))\n",
    "    encoder.add(k.layers.Dense(code_size, activation='elu'))\n",
    "    encoder.summary()\n",
    "\n",
    "    # decoder\n",
    "    decoder = k.models.Sequential()\n",
    "    decoder.add(k.layers.InputLayer((code_size,)))\n",
    "    decoder.add(k.layers.Dense(256, activation='elu'))\n",
    "    decoder.add(k.layers.Dense(512, activation='elu'))\n",
    "    decoder.add(k.layers.Dense(8192, activation='elu'))\n",
    "    decoder.add(k.layers.Reshape((8, 8, 128)))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(128, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(32, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.Conv2DTranspose(3, kernel_size=(3, 3), activation='elu', padding='same')) # Unsure about this\n",
    "    decoder.summary()\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "all_data = np.load('images_dataset.npy')\n",
    "X_train = all_data[:2723, :]\n",
    "X_test = all_data[2723:, ]\n",
    "shape = X_train[0].shape # Get from dataset\n",
    "encoder, decoder = my_autoencode(shape, code_size=128)\n",
    "inp = k.layers.Input(shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "autoencoder = k.models.Model(inputs=inp, outputs=reconstruction)\n",
    "autoencoder.compile(optimizer=\"adamax\", loss='mse')\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('hackathon_autoencoder.{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True)\n",
    "]\n",
    "\n",
    "\n",
    "# If you want to resume from a checkpoint\n",
    "#     import keras.backend as K\n",
    "#     def reset_tf_session():\n",
    "#         K.clear_session()\n",
    "#         tf.reset_default_graph()\n",
    "#         s = K.get_session()\n",
    "#         return s\n",
    "#     #### uncomment below to continue training from model checkpoint\n",
    "#     #### every time epoch counter starts at 0, so you need to track epochs manually\n",
    "#     from keras.models import load_model\n",
    "#     s = reset_tf_session()\n",
    "#     autoencoder = load_model(\"checkpoints/hackathon_autoencoder.78-508.84.h5\")  # continue after epoch 0+1\n",
    "#     encoder = autoencoder.layers[1]\n",
    "#     decoder = autoencoder.layers[2]\n",
    "\n",
    "# # Train Model\n",
    "autoencoder.fit(x=X_train, y=X_train,\n",
    "                validation_data=[X_test, X_test],\n",
    "                epochs=200,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                callbacks = callbacks\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
